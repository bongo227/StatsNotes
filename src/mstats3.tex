\cchapter{Maths Statistics 3}

\section{Geometric and Exponential Distributions}
    
    \subsection{Geometric Distributions}
        Geometric distribution is like a binomial distribution however, instead of counting the number of sucesses, it counts the number of trials until a success is obtained.
        \\\\
        Conditions for a Geometric Distribution:
        \begin{itemize}
        \item Only two possible outcomes (success or failure).
        \item Trials are independent.
        \item Probabilty of success is the same for each trial.
        \end{itemize}
        The probability density function for a Geometric Distribution is as follows:
        \begin{align*}
        P(x) &= p(1-p)^{x-1}
        \end{align*}
        Which makes sence since the probability of a success is the $x$ times the probability of failure times the single probability of success.
        \\\\
        The mean of a Geometric Distribution is dirivied as follows.
        \begin{align*}
        \text{Let }q &= 1-p                     \\
        E(X) &= \sum{x_i p_i}                   \\
             &= p + 2pq + 3pq^2 + 4pq^3 + ...   \\
             &= p(1 + 2q + 3q^2 + 4q^3 + ...)   \\
             &= p(1-q)^{-2}                     \\
             &= \dfrac{p}{p^2}                  \\
             &= \dfrac{1}{p}
        \end{align*}
        \\\\
        And the varience can be found using $E(X^2) - E(X)^2 = Var(X)$
        \begin{align*}
        E(X^2) &= \dfrac{2-p}{p^2} \text{  (using the method above)}    \\
        Var(X) &= \dfrac{2-p}{p^2} - \left(\dfrac{1}{p}\right)^2        \\
               &= \dfrac{2-p-1}{p^2}                                    \\
               &= \dfrac{1-p}{p^2}                                      \\
        \end{align*}

        \begin{example}
        {
            An unbiased tetrahedral die has faces marked 1 to 4. When it is thrown on a table the score is the number on the face that is in contact with the table.
        }

            \begin{step}{Calculate the probability that it takes more than two throws to obtain a score of 4}
                \begin{align*}
                    p &= \dfrac{1}{4}                                                        \\
                    q &= \dfrac{3}{4}                                                        \\
                P(X > 2) &= 1 - P(X \leq 2)                                                     \\
                        &= 1 - \left(\dfrac{1}{4} + \dfrac{1}{4} \times \dfrac{3}{4}\right)    \\
                        &= \dfrac{9}{16}
                \end{align*}
            \end{step}

            \begin{step}{The number of throws, $Y$, that it takes for two diffrent scores to occor at least once is given by $Y = 1 + X$, where $X$ has a geometric distribution with parameter $\dfrac{3}{4}$. Determine values for $E(Y)$ and $Var(Y)$.}
                \begin{align*}
                E(Y) &= E(1+X)                          \\
                    &= E(1) + E(X)                     \\
                    &= 1 + \dfrac{1}{\frac{4}{3}}      \\
                    &= \dfrac{7}{3}                    
                \end{align*}

                \begin{align*}
                Var(Y) &= Var(1+X)                                  \\
                    &= Var(1) + Var(X)                           \\
                    &= 0 + \dfrac{\frac{1}{4}}{(\frac{3}{4})^2}  \\
                    &= \dfrac{4}{9}
                \end{align*}
            \end{step}
        \end{example}

    \subsection{Exponential Distributions}
        (See Exponential Distribution section)

\section{Estimators}
    \paragraph{Sample Statistic} A function of observed data, used to estimate parameters (such as mean and varience).

    \paragraph{Sampling Distribution} The probability distribution for a statistic based on random samples.

    \paragraph{Population Parameter} A statistical measure for a population, used within a general distribution function such a the normal distribution.

    \paragraph{Estimators} A function to estimate a population parameter from some samples.

    \paragraph{Estimates} The value yielded by an estimator.

    \paragraph{Unbiased estimator} If $T$ is the estimator for a parameter $\theta$ and $E(T) = \theta$ then $T$ is an unbiased estimator for $\theta$.

    \paragraph{Consistency} Estimators are consistent if their variation approaches zero as the sample size approaches infinity.

    \paragraph{Efficiency} A measure of quaility of an estimator, a more effiecent estimator needs fewer observations than a less efficent one.

    \paragraph{Relative Efficiency} The ratio of two procedures effciencies, given by $RE(T_1 : T_2) = \dfrac{Var(T_2)}{Var(T_1)}$

    \begin{example}
    {
        The conductivity, $\gamma$, of metal wire is estimated by observing a related random variable, R, which has probability density function

        $$
        f(r) = 
        \begin{cases}
        \dfrac{2r}{\gamma^2} & 0 \leq r \leq \gamma \\
        0 & \text{otherwise}
        \end{cases}
        $$
    }

        \begin{step}{Show that $\frac{3}{2}R$ is an unbiased estimator of $\gamma$}
            \begin{align*}
            E\left(\dfrac{3}{2}R\right) &= \dfrac{3}{2} \times E(R)                                             \\
                                        &= \dfrac{3}{2} \times \int_0^\gamma r \times \dfrac{2r}{\gamma^2} dr   \\
                                        &= \int_0^\gamma \dfrac{3r^2}{\gamma^2} dr                              \\
                                        &= \left[\dfrac{r^3}{\gamma^2}\right]^\gamma_0                          \\
                                        &= \dfrac{\gamma^3}{\gamma^2} - \dfrac{0}{\gamma^2}                     \\
                                        &= \gamma 
            \end{align*}
        \end{step}

        \begin{step}{Given that the variance of $R$ is $\frac{1}{18}\gamma^2$, find, in terms of $\gamma$, the variance of $\frac{3}{2}R$}
            \begin{align*}
            Var\left(\frac{3}{2}R\right) &= \left(\dfrac{3}{2}\right)^2 \times Var(R)   \\
                                        &= \dfrac{9}{4} \times \dfrac{\gamma^2}{18}      \\
                                        &= \dfrac{\gamma^2}{8}
            \end{align*}
        \end{step}

        \begin{step}{The conductivity can also be estimated by making observations of a random varible, S, which has mean $\frac{1}{4}\gamma$ and $\frac{1}{16}\gamma^2$. The ranomd varible $T$ is defined by $T = S_1 + S_2 + S_3$, where $S_1$, $S_2$ and $S_3$ are three independent observations of $S$.\\\\Find the value of the constant $k$ such that $kT$ is an unbiased estimator of $\gamma$}
            \begin{align*}
                \gamma &= E(kT)                                                                 \\
                \gamma &= k \times E(T)                                                         \\
                \gamma &= k \times E(S_1 + S_2 + S_3)                                           \\
                \gamma &= k \times \frac{1}{4}\gamma + \frac{1}{4}\gamma + \frac{1}{4}\gamma    \\
                \gamma &= k \times \frac{3}{4}\gamma                                            \\
                k &= \frac{4}{3}
            \end{align*}
        \end{step}

        \begin{step}{Find the relative efficiency of $\frac{3}{2}R$ with respect to $kT$}
            \begin{align*}
            Var\left(\frac{4}{3}T\right) &= \left(\dfrac{4}{3}\right)^2 \times Var(T)       \\
                                         &= \dfrac{16}{9} \times Var(S_1 + S_2 + S_3)       \\
                                         &= \dfrac{16}{9} \times (\dfrac{1}{16}\gamma^2 + \dfrac{1}{16}\gamma^2 + \dfrac{1}{16}\gamma^2)                             \\
                                         &= \dfrac{16}{9} \times \dfrac{3}{16}\gamma^2      \\
                                         &= \dfrac{\gamma^2}{3}
            \end{align*}
            \begin{align*}
                RE\left(\frac{3}{2}R : \frac{4}{3}T\right) &= \dfrac{Var\left(\dfrac{4}{3}T\right)}{ Var\left(\dfrac{3}{2}R\right)} \\
                &= \dfrac{\dfrac{\gamma^2}{3}}{\dfrac{\gamma^2}{8}}      \\
                &= \dfrac{\gamma^2}{3} \times \dfrac{8}{\gamma^2}        \\
                &= \dfrac{8}{3}
            \end{align*}
        \end{step}

        \begin{step}{State, with justification, which of $\frac{3}{2}R$ and $kT$ is a preferred estimator of $\gamma$}
            $\dfrac{8}{3} > 1$ so $\dfrac{3}{2}R$ is the preffered estimator.
        \end{step}

    \end{example}

    \begin{example}
    {
        The random variable $X$ denotes the number of successes in a random sample of size $n$ from a large population in which the proportion of successes is $p$.
        \\\\
        The random varible $Y$ denotes the number of successes in an independent random sample of size $3n$ from the same large population.
    }

    \begin{step}{Write down expressions for the mean and the variance of each of $X$ and $Y$}
        X: Mean = $np$, Variance = $np(1-p)$

        Y: Mean = $3np$, Variance = $3np(1-p)$
    \end{step}

    \begin{step}
    {
        Two estimators suggested for $p$ are:
        \begin{center}
        $\hat{P}_1 = \dfrac{X + Y}{4n}$ and $\hat{P}_2 = \dfrac{1}{2}\left(\dfrac{X}{n} + \dfrac{Y}{3n}\right)$
        \end{center}
        Show that both $\hat{P}_1$ and $\hat{P}_2$ are unbiased and consistent estimators for $p$, and that $\hat{P}_1$ is the more efficent.
    }

    \begin{align*}
        E\left(\dfrac{X + Y}{4n}\right) &= \dfrac{np + 3np}{4n}     \\
        &= \dfrac{4np}{4n}                                          \\
        &= p
    \end{align*}
    
    \begin{align*}
        E\left(\dfrac{1}{2}\left(\dfrac{X}{n} + \dfrac{Y}{3n}\right)\right) &= \frac{1}{2} \times \left(\dfrac{np}{n}+\dfrac{3np}{3n}\right)    \\
        &= \frac{1}{2} \times (p + p) \\
        &= p
    \end{align*}

    \begin{align*}
        Var\left(\dfrac{X + Y}{4n}\right) &= \dfrac{npq + 3npq}{16n^2}      \\
        &= \dfrac{4pq}{16n}                                                 \\
        &= \dfrac{pq}{4n}                                                   \\
        &= \dfrac{p(1-p)}{4n}
    \end{align*}

    \begin{center}
    as $Var\left(\dfrac{X + Y}{4n}\right) \rightarrow 0, n \rightarrow \infty$
    \end{center}

    \begin{align*}
        Var\left(\dfrac{1}{2}\left(\dfrac{X}{n} + \dfrac{Y}{3n}\right)\right) &= \frac{1}{2^2} \times \left(\dfrac{npq}{n^2}+\dfrac{3npq}{9n^2}\right)        \\
        &= \frac{1}{4} \times \dfrac{12npq}{9n^2}   \\
        &= \dfrac{pq}{3n}                           \\
        &= \dfrac{p(1-p)}{3n}
    \end{align*}

    \begin{center}
    as $Var\left(\dfrac{1}{2}\left(\dfrac{X}{n} + \dfrac{Y}{3n}\right)\right) \rightarrow 0, n \rightarrow \infty$
    \end{center}

    \begin{align*}
    RE(\hat{P}_1 : \hat{P}_2) &= \dfrac{Var(\hat{P}_2)}{Var(\hat{P}_1)}  \\
    &= \dfrac{\dfrac{p(1-p)}{3n}}{\dfrac{p(1-p)}{4n}}                               \\
    &= \frac{4}{3}                                                                  \\
    \end{align*}

    $\dfrac{4}{3} > 1$ so $\hat{P}_1$ is the best estimate for parameter $p$.

    \end{step}

    \end{example}

\section{Estimation}
\section{Hypothesis Testing}
\section{Chi-Squared ($\chi^2$) Goodness of Fit Testing}
